experiment:
  number_of_epochs: 300
data:
  data: classification.datasets.ImageNet
  sampling:
    batch_size: 1024
  transforms:
    train: !!python/tuple
    - torchvision.transforms.RandomResizedCrop:
        size: 224
        scale: !!python/tuple
        - 0.8
        - 1.0
    - torchvision.transforms.RandomHorizontalFlip
    - classification.transforms.RandAugment
    - classification.transforms.OneHotEncoding
    - classification.transforms.MixupOrCutMix:
        mixup_params:
          distribution: uniform
          probability: 0.8
    - torchvision.transforms.RandomErasing:
        p: 0.25
    val: !!python/tuple
    - torchvision.transforms.Resize:
        size: 224
    - torchvision.transforms.CenterCrop:
        size: 224
    - classification.transforms.OneHotEncoding
model:
  model.FeedForwardModel:
    layers: !!python/tuple
    - segmentation.models.UNet_encoder:
        basic_block:
          segmentation.models.blocks.SwinTransformerBlock:
            img_size: 224
            patch_size: 4
            num_heads_layers: !!python/tuple
            - 3
            - 6
            - 12
            - 24
        channels: !!python/tuple
        - 96
        - 192
        - 384
        - 768
        change_channel_in_block: false
        depth: 3
        width: 2
        downsampling:
          segmentation.models.blocks.PatchMerging:
            norm_layer: torch.nn.LayerNorm
        in_channel_size: 3
        layer_scaling: false
        trainable_downsampling: true
        linear_channel_mapping: true
        stem:
          segmentation.models.blocks.PatchEmbed:
            patch_size: 4
            bias: true
            dilation: 1
            groups: 1
            drop_rate: 0.0
        stochastic_depth_rate: 0.1
    - torch.nn.AdaptiveAvgPool2d:
        output_size: !!python/tuple
        - 1
        - 768
    - torch.nn.Flatten
    - torch.nn.Linear:
        in_features: 768
        out_features: 1000
    weight_init:
      random:
        torch.nn.Linear:
          timm.models.layers.trunc_normal_:
            std: 0.02
            bias_init: 0
        torch.nn.LayerNorm:
          torch.nn.init.constant_:
            val: 1.0
            bias_init: 0
training:
  loss:
    torch.nn.CrossEntropyLoss:
      label_smoothing: 0.1
      label_type: label
  optimizer:
    torch.optim.AdamW:
      learning_rate:
        optim.scheduler.SequentialLR:
          base: 0.001
          schedulers: !!python/tuple
          - optim.scheduler.LinearLR:
              start_factor: 1.0e-06
              end_factor: 1.0
          - optim.scheduler.CosineAnnealingLR:
              eta_min: 1.0e-06
          milestones: 20
          iteration_unit: batch
          milestones_unit: epoch
      betas: !!python/tuple
      - 0.9
      - 0.999
      weight_decay: 0.05
metrics:
  metrics: !!python/tuple
  - metrics.multiclass_metrics.Accuracy
  - metrics.multiclass_metrics.Top5Accuracy
  calculation:
    include_background_in_averages: true
    apply_softmax: true
    log_confusion_matrix: false
meta:
  technical:
    experiment_name: ImageNet_test_Swin_Transformer
    verbose: false
    maximum_actual_batch_size: 256
    log_to_neptune: epoch
    log_to_device: epoch
    number_of_cpu_threads: 8
    model_log_checkpoints: !!python/tuple
    - 1
    - 2
    - 5
    - 10
    - 20
    - 30
    - 40
    - 50
    - 75
    - 100
    - 125
    - 150
    - 175
    - 200
    - 250
    - 300
    memory_usage_limit: 120